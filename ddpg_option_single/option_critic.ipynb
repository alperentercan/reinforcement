{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Option_critic(object):\n",
    "#     def __init__(self,nb_states,nb_actions,nb_options,args):\n",
    "        \n",
    "#         self.nb_states = nb_states\n",
    "#         self.nb_actions = nb_actions\n",
    "#         self.nb_options = nb_options\n",
    "        \n",
    "#         ## Network Lookups\n",
    "#         nb_actor_in = nb_states + 1\n",
    "#         nb_actor_out = nb_actions\n",
    "#         nb_term_in = nb_states + 1\n",
    "#         nb_term_out = 1\n",
    "#         nb_qintra_in = nb_states + 1 ## qintra is Qu from the paper\n",
    "#         nb_qintra_out = 1\n",
    "        \n",
    "#         ## Initialize Networks\n",
    "        \n",
    "#         # Create Actor and Critic Network\n",
    "#         net_cfg = {\n",
    "#             'hidden1':args.hidden1, \n",
    "#             'hidden2':args.hidden2, \n",
    "#             'init_w':args.init_w\n",
    "#         }\n",
    "        \n",
    "#         ## Termination\n",
    "#         self.terminate = torch.nn.Sequential(\n",
    "#         torch.nn.Linear(nb_term_in, args.hidden1),\n",
    "#         torch.nn.ReLU(),\n",
    "#         torch.nn.Linear(args.hidden1, args.hidden2),\n",
    "#         torch.nn.ReLU(),       \n",
    "#         torch.nn.Linear(args.hidden2, nb_term_out),\n",
    "#         torch.nn.Sigmoid())\n",
    "        \n",
    "#         ## Actor\n",
    "#         self.actor = torch.nn.Sequential(\n",
    "#         torch.nn.Linear(nb_actor_in, args.hidden1),\n",
    "#         torch.nn.ReLU(),\n",
    "#         torch.nn.Linear(args.hidden1,args.hidden2),\n",
    "#         torch.nn.ReLU(),       \n",
    "#         torch.nn.Linear(args.hidden2, nb_actor_out),\n",
    "#         torch.nn.Tanh())\n",
    "\n",
    "#         ## Q-Intra      \n",
    "#         self.qintra = torch.nn.Sequential(\n",
    "#         torch.nn.Linear(nb_qintra_in, args.hidden1),\n",
    "#         torch.nn.ReLU(),\n",
    "#         torch.nn.Linear(args.hidden1,args.hidden2),\n",
    "#         torch.nn.ReLU(),       \n",
    "#         torch.nn.Linear(args.hidden2, nb_qintra_out))\n",
    "# #         torch.nn.Tanh()) \n",
    "        \n",
    "#         ### Target Networks\n",
    "#         ## Termination\n",
    "#         self.target_terminate = torch.nn.Sequential(\n",
    "#         torch.nn.Linear(nb_term_in, args.hidden1),\n",
    "#         torch.nn.ReLU(),\n",
    "#         torch.nn.Linear(args.hidden1, args.hidden2),\n",
    "#         torch.nn.ReLU(),       \n",
    "#         torch.nn.Linear(args.hidden2, nb_term_out),\n",
    "#         torch.nn.Sigmoid())\n",
    "        \n",
    "#         ## Actor\n",
    "#         self.target_actor = torch.nn.Sequential(\n",
    "#         torch.nn.Linear(nb_actor_in, args.hidden1),\n",
    "#         torch.nn.ReLU(),\n",
    "#         torch.nn.Linear(args.hidden1,args.hidden2),\n",
    "#         torch.nn.ReLU(),       \n",
    "#         torch.nn.Linear(args.hidden2, nb_actor_out),\n",
    "#         torch.nn.Tanh())\n",
    "\n",
    "#         ## Q-Intra      \n",
    "#         self.target_qintra = torch.nn.Sequential(\n",
    "#         torch.nn.Linear(nb_qintra_in, args.hidden1),\n",
    "#         torch.nn.ReLU(),\n",
    "#         torch.nn.Linear(args.hidden1,args.hidden2),\n",
    "#         torch.nn.ReLU(),       \n",
    "#         torch.nn.Linear(args.hidden2, nb_qintra_out))\n",
    "# #         torch.nn.Tanh()) \n",
    "        \n",
    "        \n",
    "\n",
    "#         self.actor_optim  = Adam(self.actor.parameters(), lr=args.prate)\n",
    "#         self.terminate_optim  = Adam(self.terminate.parameters(), lr=args.rate)\n",
    "#         self.qintra_optim  = Adam(self.qintra.parameters(), lr=args.rate)\n",
    "\n",
    "#         hard_update(self.target_actor, self.actor) # Make sure target is with the same weight\n",
    "#         hard_update(self.target_qintra, self.qintra)\n",
    "#         hard_update(self.target_terminate, self.terminate)\n",
    "\n",
    "#         self.buffer = Replay_buffer(args.rmsize,self.nb_states,self.nb_actions,self.nb_goals)\n",
    "#         self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=args.ou_theta, mu=args.ou_mu, sigma=args.ou_sigma)\n",
    "\n",
    "#         # Hyper-parameters\n",
    "#         self.batch_size = args.bsize\n",
    "#         self.tau = args.tau\n",
    "#         self.discount = args.discount\n",
    "#         self.depsilon = 1.0 / args.epsilon\n",
    "\n",
    "#         # \n",
    "#         finalEpsilon = 0.01   # value of epsilon at end of simulation. Decay rate is calculated\n",
    "#         self.epsilonDecay =  np.exp(np.log(finalEpsilon) / (args.epsilon)) # to produce this final value\n",
    "#         self.epsilon = 1.0\n",
    "#         self.s_t = None # Most recent state\n",
    "#         self.a_t = None # Most recent action\n",
    "#         self.new_episode = False\n",
    "#         self.is_training = True\n",
    "\n",
    "\n",
    "#     def update_policy(self):\n",
    "# #         # Sample batch\n",
    "# #         state_batch, action_batch, reward_batch, \\\n",
    "# #         next_state_batch, terminal_batch = self.memory.sample_and_split(self.batch_size)\n",
    "#         state_batch, action_batch, reward_batch, \\\n",
    "#         next_state_batch, terminal_batch,goal_batch = self.buffer.sample_split_batch(self.batch_size)\n",
    "        \n",
    "# #         state_batch = np.array(state_batch)\n",
    "# #         action_batch = np.array(action_batch)\n",
    "# #         reward_batch = np.array(reward_batch)\n",
    "# #         next_state_batch = np.array(next_state_batch)\n",
    "# #         terminal_batch = np.array(terminal_batch)\n",
    "\n",
    "\n",
    "# #         state_batch = torch.tensor(state_batch,dtype=torch.float32)\n",
    "# #         action_batch = torch.tensor(action_batch,dtype=torch.float32)\n",
    "# #         reward_batch = torch.tensor(reward_batch,dtype=torch.float32)\n",
    "# #         next_state_batch = torch.tensor(next_state_batch,dtype=torch.float32)\n",
    "# #         terminal_batch = torch.tensor(terminal_batch,dtype=torch.float32)\n",
    "\n",
    "#         state_batch = state_batch.clone().detach()#torch.tensor(state_batch,dtype=torch.float32)\n",
    "#         action_batch = action_batch.clone().detach()#torch.tensor(action_batch,dtype=torch.float32)\n",
    "#         reward_batch = reward_batch.clone().detach()#torch.tensor(reward_batch,dtype=torch.float32)\n",
    "#         next_state_batch = next_state_batch.clone().detach()#torch.tensor(next_state_batch,dtype=torch.float32)\n",
    "#         terminal_batch = terminal_batch.clone().detach()#torch.tensor(terminal_batch,dtype=torch.float32)\n",
    "#         goal_batch = goal_batch.clone().detach()\n",
    "# #         bdc = np.abs(bd-1)\n",
    "#         with torch.no_grad():\n",
    "#             target_critic_input = torch.cat([next_state_batch,\n",
    "#                                              self.actor_target(torch.cat([next_state_batch,goal_batch],dim=1)),\n",
    "#                                              goal_batch],dim=1)#*self.action_space_range\n",
    "#             Z = self.critic_target(target_critic_input)\n",
    "#             intermediary = self.discount*terminal_batch*Z\n",
    "#     #         print(torch.cat([intermediary,Z],1))\n",
    "\n",
    "#     #         print(Z)\n",
    "#     #         print(intermediary)\n",
    "#             Y = reward_batch + intermediary\n",
    "#         # Update Critic\n",
    "#         self.critic.zero_grad()\n",
    "#         pred = self.critic(torch.cat([state_batch,action_batch,goal_batch],dim=1))\n",
    "#         loss_critic = criterion(pred,Y)\n",
    "#         loss_critic.backward()\n",
    "#         self.critic_optim.step()\n",
    "\n",
    "#          # Update Actor\n",
    "#         self.actor.zero_grad()\n",
    "#         actor_loss = -self.critic(torch.cat([Nstate_batch,\n",
    "#                                              self.actor(torch.cat([state_batch,goal_batch],dim=1)),goal_batch],dim=1)).mean()\n",
    "#         actor_loss.backward()             \n",
    "#         self.actor_optim.step()  \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T00:54:24.834472Z",
     "start_time": "2019-11-30T00:54:24.821718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting option_critic.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile option_critic.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from buffer_option import Replay_buffer\n",
    "from util import *\n",
    "# from util import to_tensor\n",
    "# from util import hard_update\n",
    "from random_process import OrnsteinUhlenbeckProcess\n",
    "from memory import SequentialMemory\n",
    "from model import (Actor, Critic)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "class Option_critic(object):\n",
    "    def __init__(self,nb_states,nb_actions,nb_options,args):\n",
    "        \n",
    "        self.nb_states = nb_states\n",
    "        self.nb_actions = nb_actions\n",
    "        self.nb_options = nb_options\n",
    "        \n",
    "        ## Network Lookups\n",
    "        nb_actor_in = nb_states + 1\n",
    "        nb_actor_out = nb_actions\n",
    "        nb_term_in = nb_states + 1\n",
    "        nb_term_out = 1\n",
    "        nb_qintra_in = nb_states + 1 + nb_actions ## qintra is Qu from the paper\n",
    "        nb_qintra_out = 1\n",
    "        \n",
    "        ## Initialize Networks\n",
    "        \n",
    "        # Create Actor and Critic Network\n",
    "        net_cfg = {\n",
    "            'hidden1':args.hidden1, \n",
    "            'hidden2':args.hidden2, \n",
    "            'init_w':args.init_w\n",
    "        }\n",
    "        \n",
    "        ## Termination\n",
    "        self.terminate = torch.nn.Sequential(\n",
    "        torch.nn.Linear(nb_term_in, args.hidden1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(args.hidden1, args.hidden2),\n",
    "        torch.nn.ReLU(),       \n",
    "        torch.nn.Linear(args.hidden2, nb_term_out),\n",
    "        torch.nn.Sigmoid())\n",
    "        \n",
    "        ## Actor\n",
    "        self.actor = torch.nn.Sequential(\n",
    "        torch.nn.Linear(nb_actor_in, args.hidden1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(args.hidden1,args.hidden2),\n",
    "        torch.nn.ReLU(),       \n",
    "        torch.nn.Linear(args.hidden2, nb_actor_out),\n",
    "        torch.nn.Tanh())\n",
    "\n",
    "        ## Q-Intra      \n",
    "        self.qintra = torch.nn.Sequential(\n",
    "        torch.nn.Linear(nb_qintra_in, args.hidden1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(args.hidden1,args.hidden2),\n",
    "        torch.nn.ReLU(),       \n",
    "        torch.nn.Linear(args.hidden2, nb_qintra_out))\n",
    "#         torch.nn.Tanh()) \n",
    "        \n",
    "        ### Target Networks\n",
    "        ## Termination\n",
    "        self.target_terminate = torch.nn.Sequential(\n",
    "        torch.nn.Linear(nb_term_in, args.hidden1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(args.hidden1, args.hidden2),\n",
    "        torch.nn.ReLU(),       \n",
    "        torch.nn.Linear(args.hidden2, nb_term_out),\n",
    "        torch.nn.Sigmoid())\n",
    "        \n",
    "        ## Actor\n",
    "        self.target_actor = torch.nn.Sequential(\n",
    "        torch.nn.Linear(nb_actor_in, args.hidden1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(args.hidden1,args.hidden2),\n",
    "        torch.nn.ReLU(),       \n",
    "        torch.nn.Linear(args.hidden2, nb_actor_out),\n",
    "        torch.nn.Tanh())\n",
    "\n",
    "        ## Q-Intra      \n",
    "        self.target_qintra = torch.nn.Sequential(\n",
    "        torch.nn.Linear(nb_qintra_in, args.hidden1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(args.hidden1,args.hidden2),\n",
    "        torch.nn.ReLU(),       \n",
    "        torch.nn.Linear(args.hidden2, nb_qintra_out))\n",
    "#         torch.nn.Tanh()) \n",
    "        \n",
    "        \n",
    "\n",
    "        self.actor_optim  = Adam(self.actor.parameters(), lr=args.prate)\n",
    "        self.terminate_optim  = Adam(self.terminate.parameters(), lr=args.rate)\n",
    "        self.qintra_optim  = Adam(self.qintra.parameters(), lr=args.rate)\n",
    "\n",
    "        hard_update(self.target_actor, self.actor) # Make sure target is with the same weight\n",
    "        hard_update(self.target_qintra, self.qintra)\n",
    "        hard_update(self.target_terminate, self.terminate)\n",
    "\n",
    "        self.buffer = Replay_buffer(args.rmsize,self.nb_states,self.nb_actions)\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=args.ou_theta, mu=args.ou_mu, sigma=args.ou_sigma)\n",
    "\n",
    "        # Hyper-parameters\n",
    "        self.batch_size = args.bsize\n",
    "        self.tau = args.tau\n",
    "        self.discount = args.discount\n",
    "        self.depsilon = 1.0 / args.epsilon\n",
    "\n",
    "        # \n",
    "        finalEpsilon = 0.01   # value of epsilon at end of simulation. Decay rate is calculated\n",
    "        self.epsilonDecay =  np.exp(np.log(finalEpsilon) / (args.epsilon)) # to produce this final value\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_option = 0.2\n",
    "        self.s_t = None # Most recent state\n",
    "        self.a_t = None # Most recent action\n",
    "        self.w_t = None # Most recent option\n",
    "        self.new_episode = False\n",
    "        self.is_training = True\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def update_policy(self):\n",
    "#         # Sample batch\n",
    "\n",
    "        state_batch, action_batch,option_batch,reward_batch, \\\n",
    "        next_state_batch, terminal_batch = self.buffer.sample_split_batch(self.batch_size)\n",
    "        \n",
    "\n",
    "\n",
    "        state_batch = state_batch.clone().detach()\n",
    "        action_batch = action_batch.clone().detach()\n",
    "        option_batch = option_batch.clone().detach()\n",
    "        reward_batch = reward_batch.clone().detach().reshape(-1,1)\n",
    "        next_state_batch = next_state_batch.clone().detach()\n",
    "        terminal_batch = terminal_batch.clone().detach().reshape(-1,1)\n",
    "#         goal_batch = goal_batch.clone().detach()\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            target_terminate_output_onw = self.target_terminate(torch.cat([next_state_batch,option_batch],dim=1))\n",
    "            qintra_values = np.zeros((self.batch_size,self.nb_options))\n",
    "            for i in range(self.nb_options):\n",
    "                qintra_values[:,i:i+1] =(self.target_qintra(torch.cat([\n",
    "                    next_state_batch,\n",
    "                    i*torch.ones((self.batch_size,1)),\n",
    "                    self.target_actor(torch.cat([next_state_batch,i*torch.ones((self.batch_size,1))],dim=1))],dim=1)))\n",
    "\n",
    "\n",
    "\n",
    "            target_qintra_overw = torch.tensor(np.amax(qintra_values,axis=1),dtype=torch.float).reshape(-1,1)\n",
    "#             _,target_qintra_overw = torch.max(qintra_values,axis=1)\n",
    "\n",
    "#             Q =   self.target_qintra(torch.cat([\n",
    "#                                             next_state_batch,\n",
    "#                                             option_batch,\n",
    "#                                             self.target_actor(torch.cat([next_state_batch,\n",
    "#                                                                          option_batch],dim=1))],dim=1))\n",
    "\n",
    "            \n",
    "            Y =  (reward_batch + terminal_batch*self.discount*((1-target_terminate_output_onw) * \n",
    "                                             self.target_qintra(torch.cat([next_state_batch,\n",
    "                                                                            option_batch,\n",
    "                                                                            self.target_actor(torch.cat([next_state_batch,\n",
    "                                                                                                             option_batch],dim=1))],dim=1)))\n",
    "                            +terminal_batch*self.discount*(target_terminate_output_onw)*target_qintra_overw)\n",
    "            \n",
    "        # Update Qintra\n",
    "        self.qintra.zero_grad()\n",
    "        pred = self.qintra(torch.cat([state_batch,option_batch,action_batch],dim=1))\n",
    "        loss_qintra = criterion(pred,Y)\n",
    "        loss_qintra.backward()\n",
    "        self.qintra_optim.step()\n",
    "\n",
    "        # Update Actor\n",
    "        self.actor.zero_grad()\n",
    "        actor_loss = -self.qintra(torch.cat([state_batch,\n",
    "                                             option_batch,\n",
    "                                             self.actor(torch.cat([state_batch,option_batch],dim=1))],dim=1)).mean()\n",
    "        actor_loss.backward()             \n",
    "        self.actor_optim.step()  \n",
    "\n",
    "        # Update Terminate\n",
    "        self.terminate.zero_grad()\n",
    "#         self.qintra.zero_grad()\n",
    "#         self.actor.zero_grad()\n",
    "        ## Not advantage\n",
    "        advantage_batch = self.qintra(torch.cat([next_state_batch,\n",
    "                                                 option_batch,\n",
    "                                                 self.actor(torch.cat([next_state_batch,option_batch],dim=1))],dim=1))\n",
    "        terminate_loss = (self.terminate(torch.cat([next_state_batch,option_batch],dim=1))*advantage_batch).mean()\n",
    "        terminate_loss.backward()\n",
    "        self.terminate_optim.step()\n",
    "        self.soft_update()\n",
    "    \n",
    "  \n",
    "    def select_action(self, s_t, decay_epsilon=True):\n",
    "#         if self.w_t == None:\n",
    "#             if np.random.uniform() > self.epsilon_option:\n",
    "#                 print('noluyor aga')\n",
    "#                 option_qs = [self.qintra(torch.cat([to_tensor(np.append(s_t,w)),#s_t,\n",
    "# #                                                    w,\n",
    "#                                                    self.actor(to_tensor(np.append(s_t,w)))])) for w in range(self.nb_options)] \n",
    "# #                                                    self.actor(torch.cat([s_t,w],dim=1))],dim=1)) for w in range(self.nb_options)] \n",
    "#                 ind = range(self.nb_options)\n",
    "#                 self.w_t = max(ind,key=lambda x:option_qs[x])\n",
    "#             else:\n",
    "#                 self.w_t = np.random.randint(self.nb_options)\n",
    "#         else:\n",
    "        old_option = self.w_t\n",
    "        if self.terminate(to_tensor(np.append(s_t,self.w_t))) == 1:\n",
    "            if np.random.uniform() > self.epsilon_option:\n",
    "\n",
    "                option_qs = [self.qintra(torch.cat([to_tensor(np.append(s_t,w)),#s_t,\n",
    "#                                                        w,\n",
    "                                                   self.actor(to_tensor(np.append(s_t,w)))])) for w in range(self.nb_options)] \n",
    "                ind = range(self.nb_options)\n",
    "                self.w_t = max(ind,key=lambda x:option_qs[x])\n",
    "            else:\n",
    "                self.w_t = np.random.randint(self.nb_options)\n",
    "        \n",
    "        action = to_numpy(\n",
    "            self.actor(to_tensor(np.append(s_t,self.w_t)))).squeeze(0)        \n",
    "#         action = to_numpy(\n",
    "#             self.actor(torch.cat([to_tensor(np.array([s_t])),\n",
    "#                                   torch.tensor([self.w_t],dtype=torch.float)],dim=1))).squeeze(0)\n",
    "        action = action.reshape(1,-1)\n",
    "        action += self.is_training*max(self.epsilon, 0)*self.random_process.sample()\n",
    "        action = np.clip(action, -1., 1.)\n",
    "\n",
    "        if decay_epsilon:\n",
    "            self.epsilon -= self.depsilon\n",
    "        print(f\"State {s_t}, old option was {old_option}, new option is {self.w_t} and the action is {action}\")\n",
    "        self.a_t = action\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def observe(self,reward,obs,done):\n",
    "\n",
    "        if self.is_training:    \n",
    "            if self.new_episode:\n",
    "                self.new_episode = False\n",
    "                self.s_t = obs\n",
    "                \n",
    "            else:\n",
    "                if done:\n",
    "                    self.new_episode = True\n",
    "#                 print(np.array([self.w_t]).shape)\n",
    "                self.buffer.add_entry(self.s_t, self.a_t,self.w_t,reward,obs, done)\n",
    "#                 self.memory.append(self.s_t, self.a_t, reward, done)\n",
    "                self.s_t = obs\n",
    "    \n",
    "\n",
    "    def random_action(self):\n",
    "        action = np.random.uniform(-1.,1.,self.nb_actions)#*self.action_space_range\n",
    "        self.a_t = action\n",
    "        return action  \n",
    "\n",
    "    def soft_update(self):      \n",
    "        #Update target networks          \n",
    "        with torch.no_grad():\n",
    "            for i in [0,2,4]:\n",
    "                self.target_qintra[i].weight.data = (self.tau*self.qintra[i].weight.data.clone() + \n",
    "                                                     (1-self.tau) *self.target_qintra[i].weight.data.clone())\n",
    "            for i in [0,2,4]:\n",
    "                self.target_actor[i].weight.data = (self.tau*self.actor[i].weight.data.clone() + \n",
    "                                                    (1-self.tau)*self.target_actor[i].weight.data.clone())\n",
    "                           \n",
    "            for i in [0,2,4]:\n",
    "                self.target_terminate[i].weight.data = (self.tau*self.terminate[i].weight.data.clone() + \n",
    "                                                    (1-self.tau)*self.target_terminate[i].weight.data.clone())\n",
    "\n",
    "\n",
    "    def save_model(self,output):\n",
    "        return 0\n",
    "\n",
    "    def reset(self, obs):\n",
    "        self.s_t = obs\n",
    "        ## initialize option\n",
    "        if np.random.uniform() > self.epsilon_option:\n",
    "#             print(obs)\n",
    "#             print(to_tensor(np.append(obs,1)).shape)\n",
    "#             print(self.actor(to_tensor(np.append(obs,1))).shape)\n",
    "#             print(torch.cat([torch.tensor(np.append(obs,1),dtype=torch.float32),self.actor(to_tensor(np.append(obs,1)))]).shape)\n",
    "#             print(np.append(obs,1).shape)\n",
    "            option_qs = [self.qintra(torch.cat([to_tensor(np.append(obs,w),dtype=torch.float32),\n",
    "#                                                torch.tensor(w),\n",
    "                                               self.actor(to_tensor(np.append(obs,w)))])) for w in range(self.nb_options)]\n",
    "#             option_qs = [np.append(obs,w) for w in range(self.nb_options)] \n",
    "#             print(option_qs)\n",
    "            ind = range(self.nb_options)\n",
    "            self.w_t = max(ind,key=lambda x:option_qs[x])\n",
    "        else:\n",
    "            self.w_t = np.random.randint(self.nb_options)\n",
    "        self.random_process.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T23:20:27.316493Z",
     "start_time": "2019-11-28T23:20:27.310477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `np.zeros` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
